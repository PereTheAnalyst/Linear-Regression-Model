import math
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Step 1: Import dataset
df = pd.read_csv("multiple_linear_regression_dataset.csv")

# Step 2: Define the target values and training data
x_train = df['experience'].values
y_train = df['income'].values
fx_train = np.array([1.0, 2.0])   # Featured Data
fy_train = np.array([300.0, 500.0])

# Step 3: Compute the cost function
def compute_costs(x, y, w, b):
    m = len(x)  # Number of data points
    cost = np.sum((w * x + b - y) ** 2)
    total_cost = (1 / (2 * m)) * cost
    return total_cost

# Step 4: Compute the gradients for gradient descent
def compute_gradient(x, y, w, b):
    m = len(x) 
    dj_dw = np.sum((w * x + b - y) * x)
    dj_db = np.sum(w * x + b - y)
    dj_dw /= m
    dj_db /= m
    return dj_dw, dj_db

# Step 5: Perform gradient descent
def gradient_descent(x, y, w_init, b_init, alpha, num_iters, cost_function, gradient_function):
    j_history = []
    p_history = []
    b = b_init
    w = w_init

    for i in range(num_iters):
        dj_dw, dj_db = gradient_function(x, y, w, b)
        b -= alpha * dj_db
        w -= alpha * dj_dw
        
        if i < 10000:
            j_history.append(cost_function(x, y, w, b))
            p_history.append([w, b])
        
        if i % math.ceil(num_iters / 10) == 0:
            print(f"Iteration {i:4}: Cost {j_history[-1]:0.2e}, dj_dw: {dj_dw:0.3e}, dj_db: {dj_db:0.3e}, "
                  f"w: {w:0.3e}, b: {b:0.5e}")

    return w, b, j_history, p_history

# Step 6: Make predictions
def predict(w, b, x):
    return np.multiply(w, x) + b

# Initialize parameters
w_init = 0
b_init = 0

# Set gradient descent settings
iterations = 10000
learning_rate = 1.0e-2

# Perform gradient descent
w_final, b_final, j_hist, p_hists = gradient_descent(x_train, y_train, w_init, b_init, learning_rate, iterations,
                                                     compute_costs, compute_gradient)

print(f"(w, b) found by gradient descent: ({w_final:8.4f}, {b_final:8.4f})")

# Step 7: Plot the contour and gradient descent path
fig, ax = plt.subplots(1, 1, figsize=(12, 6))

# Create a grid of w and b values for contour plotting
w_vals = np.linspace(-500, 500, 100)
b_vals = np.linspace(-5000, 5000, 100)
W, B = np.meshgrid(w_vals, b_vals)

# Compute the cost for each combination of w and b values
Z = np.zeros_like(W)
for i in range(len(w_vals)):
    for j in range(len(b_vals)):
        Z[j, i] = compute_costs(x_train, y_train, W[j, i], B[j, i])

# Plot the contour
contour = ax.contour(W, B, Z, levels=20)

# Plot the gradient descent path
w_vals = [p[0] for p in p_hists]
b_vals = [p[1] for p in p_hists]
ax.plot(w_vals, b_vals, 'ro-', markersize=3)

# Set labels and title for the plot
ax.set_xlabel('w')
ax.set_ylabel('b')
ax.set_title('Contour Plot with Gradient Descent Path')

# Show the plot
plt.show()

# Step 8: Compute the total cost with the final values
x = x_train
y = y_train
w = w_final
b = b_final
total_cost = compute_costs(x, y, w, b)

# Step 9: Plot the data points
plt.figure(figsize=(8, 6))
plt.scatter(x_train, y_train, color='b', label='Data Points')
plt.xlabel('Experience')
plt.ylabel('Income')
plt.title('Scatter Plot of Data')
plt.legend()
plt.show()

print("Total cost:", total_cost)

# Step 10: Make predictions for sample values
experience = [3.5, 5.3, 8.7]
for years in experience:
    prediction = predict(w_final, b_final, years)
    print(f"{years} years of experience prediction: {prediction:.1f} Thousand dollars")

# Step 11: Make predictions for user input
user_input = float(input("Enter the years of experience: "))
user_prediction = predict(w_final, b_final, user_input)
print(f"Your {user_input} years of experience prediction: {user_prediction:.1f} Thousand dollars")
